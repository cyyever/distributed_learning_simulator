---
dataset_name: imdb
model_name: TransformerClassificationModel
distributed_algorithm: fed_obd
optimizer_name: SGD
worker_number: 10
round: 100
learning_rate_scheduler_name: CosineAnnealingLR
epoch: 5
batch_size: 64
learning_rate: 0.01
endpoint_kwargs:
  server:
    weight: 0.001
  worker:
    weight: 0.001
algorithm_kwargs:
  second_phase_epoch: 10
  dropout_rate: 0.9
  random_client_number: 5
dataset_kwargs:
  max_len: 300
model_kwargs:
  max_len: 300
  word_vector_name: glove.6B.100d
  num_encoder_layer: 2
  d_model: 100
  nhead: 5
